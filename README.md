# GWAS Nextflow Pipeline

## Table of Contents

- [Introduction](#introduction)
- [Requirements](#requirements)
- [Installation](#installation)
- [Configuration](#configuration)
- [Pipeline Structure](#pipeline-structure)
- [Running the Pipeline](#running-the-pipeline)
- [Required Input Files](#required-input-files)
- [Output Files](#output-files)


## Introduction

This repository contains a Nextflow pipeline for Genome-Wide Association Studies (GWAS). The pipeline performs data quality control, data stratification by ethnic groups, and common variant analysis for *the use of Temus case study interview*.

## Requirements

- Nextflow >= 24.04.3
- Docker for containerized execution

## Installation

### Clone the repository

```bash
git clone https://github.com/phingchian/temus_case_study.git
cd temus_case_study
mkdir dataset
```

### Download the case study dataset

1. Download the [Dataset](https://www.kaggle.com/datasets/103b608eea3a94c5c98260738d80039c5573eb7f80dc0a8e4f865cb90fbc6ea4?resource=download) from Kaggle.
2. Unzip the files into `dataset` folder.


### Building the Docker Image using the Dockerfile

The Docker Image contains `plink2` and Python libraries necessary to run the pipeline.

```bash
cd docker
docker build -t temus .
cd ..
```

## Configuration

The pipeline is configured using a `nextflow.config` file. You can modify this file to change parameters, input/output paths, and other settings.

## Pipeline Structure

- Nextflow
    - `main.nf`: The main Nextflow script that defines the workflow.
    - `nextflow.config`: Configuration file for setting parameters and paths.
- Docker
    - `Dockerfile`: Dockerfile for building the container image containing all the necessary libraries
    - `env.yaml`: Conda environment configuration file.
- Python helper scripts
    - `plot_missingness.py`: Python script for plotting missing data.
    - `split_ethnicity.py`: Python script for stratifying population data by ethnic groups.
    - `common_variant.py`: Python script for common variant analysis.

## Running the Pipeline

The pipeline encompasses the following steps:

```
--> Remove high-missingness SNPs/Individuals 
--> Plot missingness 
--> Split genotype files by ethnicity 
--> Perform GWAS on each ethnicity 
--> Identify common variants for each phenotype 
--> Plot Manhattan and QQ diagrams on phenotypes with common significant variants
```

To run the pipeline:

```bash
nextflow run main.nf
```

## Required Input Files

1. Genotypic data file
    - `.bed` file: Binary file for genotype information
    - `.bim` file: BIM file containing variant information, 6 columns [`Chr`, `SNP`, `GD`, `BPP`, `Allele1`, `Allele2`]
    - `.fam` file: FAM file containing family and individual ID, 6 columns [`FID`, `IID`, `PID`, `MID`, `Sex`, `Phenotype`]
2. Phenotype file
    - `.txt` file containing data with the following columns [`FID`, `IID`, `Y1`, `Y2`, `Y3`, ... `YN`]. Y1 to YN are phenotypic values and their columns names are not fixed.
3. Ethnicity information file
    - `.txt` file containing information about the ethnic group of each family, 2 columns [`FID`, `ETHNIC_GROUP`]

## Output Files

Output files generated by the pipeline are located in Nextflow `work` directory. The final output files at important checkpoints are captured in the `results` folder:
- `results/*_postQC.bed`: Filtered BED file with high-missingness entries removed
- `results/*_postQC.bim`: Filtered BIM file with high-missingness entries removed
- `results/*_postQC.fam`: Filtered FAM file with high-missingness entries removed
- `results/eth_*.txt`: Ethnicity files with FID belonging to a single ethnicity group
- `results/missing_plot.png`: Histograms to show the distributions of missing data by SNP and by ID
- `results/gwas_output/*eth_[^.].[^.].glm.linear.png`: Manhattan/QQ plots generated for a phenotype in all ethnic groups. *Only applicable to the phenotypes that have common variants in all ethnicity groups*
- `results/gwas_output/*[^.].glm.linear.csv`: Tabular file showing p-values of the significant common variants for a given phenotype



